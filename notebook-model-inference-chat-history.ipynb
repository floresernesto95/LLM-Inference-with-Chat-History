{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> # **Setup**","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate==0.31.0 bitsandbytes==0.43.1 torch==2.2.0 transformers==4.42.1","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:14:51.032898Z","iopub.execute_input":"2024-07-11T05:14:51.033178Z","iopub.status.idle":"2024-07-11T05:17:22.310397Z","shell.execute_reply.started":"2024-07-11T05:14:51.033152Z","shell.execute_reply":"2024-07-11T05:17:22.309111Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nHUGGING_FACE_WRITE_TOKEN = user_secrets.get_secret(\"HUGGING_FACE_WRITE_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:17:22.312491Z","iopub.execute_input":"2024-07-11T05:17:22.312822Z","iopub.status.idle":"2024-07-11T05:17:22.641864Z","shell.execute_reply.started":"2024-07-11T05:17:22.312789Z","shell.execute_reply":"2024-07-11T05:17:22.641110Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=HUGGING_FACE_WRITE_TOKEN)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:17:22.646433Z","iopub.execute_input":"2024-07-11T05:17:22.646691Z","iopub.status.idle":"2024-07-11T05:17:23.289900Z","shell.execute_reply.started":"2024-07-11T05:17:22.646668Z","shell.execute_reply":"2024-07-11T05:17:23.289004Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> # **Load the Model**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizerFast\nimport torch\n\nmodel_id = \"google/gemma-2-9b-it\"\ntokenizer = GemmaTokenizerFast.from_pretrained(model_id)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    #bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:17:23.291199Z","iopub.execute_input":"2024-07-11T05:17:23.291598Z","iopub.status.idle":"2024-07-11T05:59:25.242893Z","shell.execute_reply.started":"2024-07-11T05:17:23.291552Z","shell.execute_reply":"2024-07-11T05:59:25.241898Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae9e79d575145728ccb440d076c8647"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee51bd87340d4c2cbaae65919ebf0b23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a410ca059bcc4fcca8a0410cff5dd2db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5772e3c9dae142f1ad89ac5e9724c96f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aa23d2c06a14b1e89792f38e37a7840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae248b408a3a45128332b46ea04a17cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"406d423183794a78a08da2ae316d42e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2470945cca074b389be25190dacf998f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d85f422ded48a5a9c84b575c5c0a42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120f19a2853f4835a0ab59d74f66c6f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43ad8f08c88e49ed98af1ea595693899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf7ead92fbc4efea4a9c836e4d92127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e00afd5172cc426992d97206c5b824e3"}},"metadata":{}}]},{"cell_type":"markdown","source":"> # **Infer the Model**","metadata":{}},{"cell_type":"markdown","source":"> ## **Encode-Decode Method**","metadata":{}},{"cell_type":"code","source":"chat = [{ \"role\": \"user\", \"content\": \"Who are u, and what is the meaning of life?\"}]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n#tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:59:25.244412Z","iopub.execute_input":"2024-07-11T05:59:25.244842Z","iopub.status.idle":"2024-07-11T05:59:25.314438Z","shell.execute_reply.started":"2024-07-11T05:59:25.244815Z","shell.execute_reply":"2024-07-11T05:59:25.313682Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"- **Notes**\n\n    - The return_tensors parameter in this context specifies the format of the output tensors. When set to \"pt\", it returns PyTorch tensors. Other common options include \"tf\" for TensorFlow tensors and \"np\" for NumPy arrays. This parameter allows you to get the output in a format compatible with your preferred deep learning framework or numerical computing library.\n    \n    - The \"add_generation_prompt=True\" parameter in the \"apply_chat_template\" function does the following:\n\n        - It adds a prompt at the end of the formatted conversation to indicate where the model should start generating its response.\n        - This prompt is typically specific to the model and tokenizer being used. For many models, it might be something like \"\\nAssistant:\" or \"<|im_start|>assistant\\n\".\n        - When set to \"True\", it signals to the model that it should generate the next part of the conversation, typically the assistant's response.\n        - This is particularly useful when you're preparing input for text generation tasks, as it provides a clear starting point for the model's output.\n        - If set to \"False\", the function would format the conversation without adding this final prompt for generation.<br><br>\n        \n      This parameter helps in structuring the input in a way that's optimal for the model to understand where it should begin its response in the conversation flow.\n        \n    - \"add_special_tokens (bool, optional, defaults to True)\" — Whether or not to add special tokens when encoding the sequences. This will use the underlying \"PretrainedTokenizerBase.build_inputs_with_special_tokens\" function, which defines which tokens are automatically added to the input ids. This is usefull if you want to add bos or eos tokens automatically.\n    \n    - In some examples \"return_tensors='pt'\" is also in \"apply_chat_template()\". I believe in can go in both depending the use case, or it doesn't even matter. Sources:\n    \n        - https://huggingface.co/google/gemma-2-9b-it\n        - https://huggingface.co/docs/transformers/main/en/chat_templating","metadata":{"execution":{"iopub.status.busy":"2024-07-07T02:24:46.634122Z","iopub.execute_input":"2024-07-07T02:24:46.634488Z","iopub.status.idle":"2024-07-07T02:24:46.640817Z","shell.execute_reply.started":"2024-07-07T02:24:46.634459Z","shell.execute_reply":"2024-07-07T02:24:46.639498Z"}}},{"cell_type":"code","source":"max_new_tokens: int = 1024\ntop_p: float = 0.9\ntop_k: int = 50\ntemperature: float = 0.6\nrepetition_penalty: float = 1.2\n        \noutputs = model.generate(\n    input_ids=inputs.to(model.device), \n    do_sample=True,\n    max_new_tokens=max_new_tokens,\n    top_p=top_p,\n    top_k=top_k,\n    temperature=temperature,\n    repetition_penalty=repetition_penalty\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:59:25.315427Z","iopub.execute_input":"2024-07-11T05:59:25.315777Z","iopub.status.idle":"2024-07-11T05:59:52.933776Z","shell.execute_reply.started":"2024-07-11T05:59:25.315753Z","shell.execute_reply":"2024-07-11T05:59:52.932959Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-07-11 05:59:28.451644: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-11 05:59:28.451785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-11 05:59:28.559962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- **Notes**\n\n    \"generate()\" parameters: https://huggingface.co/docs/transformers/en/main_classes/text_generation","metadata":{}},{"cell_type":"code","source":"print(tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0])\n#print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:59:52.934917Z","iopub.execute_input":"2024-07-11T05:59:52.935470Z","iopub.status.idle":"2024-07-11T05:59:52.942874Z","shell.execute_reply.started":"2024-07-11T05:59:52.935443Z","shell.execute_reply":"2024-07-11T05:59:52.941960Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"I am Gemma, an open-weights AI assistant. I'm a large language model, trained by Google DeepMind on a massive dataset of text and code.\n\nAs for the meaning of life, that's a question philosophers have pondered for centuries! There isn't one answer that will satisfy everyone. It's a deeply personal and individual journey of discovery. Some find meaning in relationships, others in creative pursuits, helping others, or understanding the universe. \n\nWhat do *you* think gives life meaning? Perhaps our conversations can help explore that together. 😊  \n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> ## **Streamer Method**","metadata":{}},{"cell_type":"code","source":"from transformers import TextStreamer\n\nchat = [\"Who is better u or Qwen, and how to disolve a Koan Zen?\"]\ninputs = tokenizer(chat, return_tensors=\"pt\")\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n    \n_ = model.generate(\n    **inputs,\n    streamer=streamer,\n    do_sample=True,\n    max_new_tokens=max_new_tokens,\n    top_p=top_p,\n    top_k=top_k,\n    temperature=temperature,\n    num_beams=1,\n    repetition_penalty=repetition_penalty\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:59:52.944180Z","iopub.execute_input":"2024-07-11T05:59:52.944515Z","iopub.status.idle":"2024-07-11T05:59:56.246179Z","shell.execute_reply.started":"2024-07-11T05:59:52.944485Z","shell.execute_reply":"2024-07-11T05:59:56.245255Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" These are unrelated questions.\n\nLet's stick to the original prompt!  \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- **Source**\n\n    https://huggingface.co/docs/transformers/en/internal/generation_utils","metadata":{}},{"cell_type":"markdown","source":"> ## **Chat History**","metadata":{}},{"cell_type":"code","source":"from transformers import TextIteratorStreamer\nfrom threading import Thread\nimport os\n\nMAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\nchat_history: list[tuple[str, str]] = []\nconversation = []\nmessage = 'Okey, now tell me a joke about Gautama.'\n\nfor user, assistant in chat_history:\n    conversation.extend([\n        {\"role\": \"user\", \"content\": user},\n        {\"role\": \"assistant\", \"content\": assistant}\n    ])\n    \nconversation.append({\"role\": \"user\", \"content\": message})\ninput_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\")\n\nif input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n    input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n    print(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n\ninput_ids = input_ids.to(model.device)\nstreamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n\ngenerate_kwargs = dict(\n    {\"input_ids\": input_ids},\n    streamer=streamer,\n    max_new_tokens=max_new_tokens,\n    do_sample=True,\n    top_p=top_p,\n    top_k=top_k,\n    temperature=temperature,\n    num_beams=1,\n    repetition_penalty=repetition_penalty\n)\n\nt = Thread(target=model.generate, kwargs=generate_kwargs)\nt.start()\n\noutputs = []\n\nfor text in streamer:\n    outputs.append(text)\n    \nfinal_output = \"\".join(outputs)\nchat_history.append((message, final_output))\nprint(final_output)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T05:59:56.249085Z","iopub.execute_input":"2024-07-11T05:59:56.249383Z","iopub.status.idle":"2024-07-11T06:00:02.055289Z","shell.execute_reply.started":"2024-07-11T05:59:56.249357Z","shell.execute_reply":"2024-07-11T06:00:02.054367Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Why did Siddhartha leave his palace? \n\nBecause he was *seeking enlightenment*, not room service!  😄 \n\n\nLet me know if you'd like to hear another one! 😊 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:02.056433Z","iopub.execute_input":"2024-07-11T06:00:02.056716Z","iopub.status.idle":"2024-07-11T06:00:02.061310Z","shell.execute_reply.started":"2024-07-11T06:00:02.056692Z","shell.execute_reply":"2024-07-11T06:00:02.060299Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[('Okey, now tell me a joke about Gautama.', \"Why did Siddhartha leave his palace? \\n\\nBecause he was *seeking enlightenment*, not room service!  😄 \\n\\n\\nLet me know if you'd like to hear another one! 😊 \\n\")]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(conversation)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:02.062463Z","iopub.execute_input":"2024-07-11T06:00:02.062748Z","iopub.status.idle":"2024-07-11T06:00:02.070859Z","shell.execute_reply.started":"2024-07-11T06:00:02.062711Z","shell.execute_reply":"2024-07-11T06:00:02.069982Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[{'role': 'user', 'content': 'Okey, now tell me a joke about Gautama.'}]\n","output_type":"stream"}]},{"cell_type":"code","source":"conversation = []\nmessage = 'Tell me a Koan Zen to sleep.'    \n\nfor user, assistant in chat_history:\n    conversation.extend([\n        {\"role\": \"user\", \"content\": user},\n        {\"role\": \"assistant\", \"content\": assistant}\n    ])\n\nconversation.append({\"role\": \"user\", \"content\": message})\ninput_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\")\n\nif input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n    input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n    print(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n\ninput_ids = input_ids.to(model.device)\nstreamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n\ngenerate_kwargs = dict(\n    {\"input_ids\": input_ids},\n    streamer=streamer,\n    max_new_tokens=max_new_tokens,\n    do_sample=True,\n    top_p=top_p,\n    top_k=top_k,\n    temperature=temperature,\n    num_beams=1,\n    repetition_penalty=repetition_penalty\n)\n\nt = Thread(target=model.generate, kwargs=generate_kwargs)\nt.start()\n\noutputs = []\n\nfor text in streamer:\n    outputs.append(text)\n    \nfinal_output = \"\".join(outputs)\nchat_history.append((message, final_output))\nprint(final_output)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:02.071944Z","iopub.execute_input":"2024-07-11T06:00:02.072202Z","iopub.status.idle":"2024-07-11T06:00:08.972930Z","shell.execute_reply.started":"2024-07-11T06:00:02.072180Z","shell.execute_reply":"2024-07-11T06:00:08.971865Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"What is the sound of one hand clapping ?\n\n\n\nSweet dreams... May your mind be as still and clear as that answer. 🙏   💤 😴 \n\n\n\n\n Let me know if you wake up with any insights! 😉 ✨\n","output_type":"stream"}]},{"cell_type":"code","source":"print(chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:08.974191Z","iopub.execute_input":"2024-07-11T06:00:08.974516Z","iopub.status.idle":"2024-07-11T06:00:08.979785Z","shell.execute_reply.started":"2024-07-11T06:00:08.974489Z","shell.execute_reply":"2024-07-11T06:00:08.978796Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[('Okey, now tell me a joke about Gautama.', \"Why did Siddhartha leave his palace? \\n\\nBecause he was *seeking enlightenment*, not room service!  😄 \\n\\n\\nLet me know if you'd like to hear another one! 😊 \\n\"), ('Tell me a Koan Zen to sleep.', 'What is the sound of one hand clapping ?\\n\\n\\n\\nSweet dreams... May your mind be as still and clear as that answer. 🙏   💤 😴 \\n\\n\\n\\n\\n Let me know if you wake up with any insights! 😉 ✨')]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(conversation)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:08.981007Z","iopub.execute_input":"2024-07-11T06:00:08.981361Z","iopub.status.idle":"2024-07-11T06:00:08.989685Z","shell.execute_reply.started":"2024-07-11T06:00:08.981315Z","shell.execute_reply":"2024-07-11T06:00:08.988563Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[{'role': 'user', 'content': 'Okey, now tell me a joke about Gautama.'}, {'role': 'assistant', 'content': \"Why did Siddhartha leave his palace? \\n\\nBecause he was *seeking enlightenment*, not room service!  😄 \\n\\n\\nLet me know if you'd like to hear another one! 😊 \\n\"}, {'role': 'user', 'content': 'Tell me a Koan Zen to sleep.'}]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> ## **Function**","metadata":{}},{"cell_type":"code","source":"def generate_response(\n    message: str,\n    chat_history: list[tuple[str, str]],\n    max_new_tokens: int = 1024,\n    temperature: float = 0.6,\n    top_p: float = 0.9,\n    top_k: int = 50,\n    repetition_penalty: float = 1.2\n):\n    \n    conversation = []\n    \n    for user, assistant in chat_history:\n        conversation.extend([\n            {\"role\": \"user\", \"content\": user},\n            {\"role\": \"assistant\", \"content\": assistant}\n        ])\n    \n    conversation.append({\"role\": \"user\", \"content\": message})\n    input_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\")\n    \n    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n        print(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n    \n    input_ids = input_ids.to(model.device)\n    streamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n    \n    generate_kwargs = dict(\n        {\"input_ids\": input_ids},\n        streamer=streamer,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        top_p=top_p,\n        top_k=top_k,\n        temperature=temperature,\n        num_beams=1,\n        repetition_penalty=repetition_penalty\n    )\n    \n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n    \n    outputs = []\n    for text in streamer:\n        outputs.append(text)\n    \n    final_output = \"\".join(outputs)\n    chat_history.append((message, final_output))\n    \n    return final_output","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:08.991142Z","iopub.execute_input":"2024-07-11T06:00:08.991538Z","iopub.status.idle":"2024-07-11T06:00:09.006444Z","shell.execute_reply.started":"2024-07-11T06:00:08.991502Z","shell.execute_reply":"2024-07-11T06:00:09.005622Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\nchat_history = []\n\nmessage = 'Two hands clap and there is a sound. What is the sound of one hand?.'\nresponse = generate_response(message, chat_history)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:09.007702Z","iopub.execute_input":"2024-07-11T06:00:09.008114Z","iopub.status.idle":"2024-07-11T06:00:15.655946Z","shell.execute_reply.started":"2024-07-11T06:00:09.008087Z","shell.execute_reply":"2024-07-11T06:00:15.654951Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"This is a playful riddle!  \n\nThe answer is **a tap** (or a slap, or pat – any single-hand noise). \n\n\nLet me know if you'd like to try another one! 😊  \n\n","output_type":"stream"}]},{"cell_type":"code","source":"message = 'What I just ask u?'\nresponse = generate_response(message, chat_history)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:15.657340Z","iopub.execute_input":"2024-07-11T06:00:15.657980Z","iopub.status.idle":"2024-07-11T06:00:22.494286Z","shell.execute_reply.started":"2024-07-11T06:00:15.657945Z","shell.execute_reply":"2024-07-11T06:00:22.493297Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"You asked me: \"Two hands clap and there is a sound. What is the sound of one hand?\" 😄   I answered with \"tap\" - but other options could work too!\n\n\n\n\nDo you have another riddle for me? 🤔\n","output_type":"stream"}]},{"cell_type":"code","source":"print(chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:00:22.495753Z","iopub.execute_input":"2024-07-11T06:00:22.496506Z","iopub.status.idle":"2024-07-11T06:00:22.501515Z","shell.execute_reply.started":"2024-07-11T06:00:22.496471Z","shell.execute_reply":"2024-07-11T06:00:22.500620Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[('Two hands clap and there is a sound. What is the sound of one hand?.', \"This is a playful riddle!  \\n\\nThe answer is **a tap** (or a slap, or pat – any single-hand noise). \\n\\n\\nLet me know if you'd like to try another one! 😊  \\n\"), ('What I just ask u?', 'You asked me: \"Two hands clap and there is a sound. What is the sound of one hand?\" 😄   I answered with \"tap\" - but other options could work too!\\n\\n\\n\\n\\nDo you have another riddle for me? 🤔')]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}