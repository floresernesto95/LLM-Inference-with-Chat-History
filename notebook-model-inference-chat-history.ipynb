{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> # **Setup**","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate==0.31.0 bitsandbytes==0.43.1 torch==2.2.0 transformers==4.42.1","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:53:50.223758Z","iopub.execute_input":"2024-07-07T23:53:50.224077Z","iopub.status.idle":"2024-07-07T23:56:23.830183Z","shell.execute_reply.started":"2024-07-07T23:53:50.224052Z","shell.execute_reply":"2024-07-07T23:56:23.829013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nHUGGING_FACE_WRITE_TOKEN = user_secrets.get_secret(\"HUGGING_FACE_WRITE_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:56:23.832435Z","iopub.execute_input":"2024-07-07T23:56:23.832798Z","iopub.status.idle":"2024-07-07T23:56:24.063493Z","shell.execute_reply.started":"2024-07-07T23:56:23.832751Z","shell.execute_reply":"2024-07-07T23:56:24.062814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=HUGGING_FACE_WRITE_TOKEN)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:56:24.064383Z","iopub.execute_input":"2024-07-07T23:56:24.064630Z","iopub.status.idle":"2024-07-07T23:56:24.605826Z","shell.execute_reply.started":"2024-07-07T23:56:24.064608Z","shell.execute_reply":"2024-07-07T23:56:24.604911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # **Load the Model**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizerFast\nimport torch\n\nmodel_id = \"google/gemma-2-9b-it\"\ntokenizer = GemmaTokenizerFast.from_pretrained(model_id)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    #bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:56:24.606891Z","iopub.execute_input":"2024-07-07T23:56:24.607152Z","iopub.status.idle":"2024-07-07T23:59:07.079704Z","shell.execute_reply.started":"2024-07-07T23:56:24.607129Z","shell.execute_reply":"2024-07-07T23:59:07.078974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # **Infer the Model**","metadata":{}},{"cell_type":"markdown","source":"> ## **Encode-Decode Method**","metadata":{}},{"cell_type":"code","source":"chat = [{ \"role\": \"user\", \"content\": \"Who are u?\"}]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n#tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:59:07.082521Z","iopub.execute_input":"2024-07-07T23:59:07.082945Z","iopub.status.idle":"2024-07-07T23:59:07.145102Z","shell.execute_reply.started":"2024-07-07T23:59:07.082919Z","shell.execute_reply":"2024-07-07T23:59:07.144227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Notes**\n\n    - The return_tensors parameter in this context specifies the format of the output tensors. When set to \"pt\", it returns PyTorch tensors. Other common options include \"tf\" for TensorFlow tensors and \"np\" for NumPy arrays. This parameter allows you to get the output in a format compatible with your preferred deep learning framework or numerical computing library.\n    \n    - The \"add_generation_prompt=True\" parameter in the \"apply_chat_template\" function does the following:\n\n        - It adds a prompt at the end of the formatted conversation to indicate where the model should start generating its response.\n        - This prompt is typically specific to the model and tokenizer being used. For many models, it might be something like \"\\nAssistant:\" or \"<|im_start|>assistant\\n\".\n        - When set to \"True\", it signals to the model that it should generate the next part of the conversation, typically the assistant's response.\n        - This is particularly useful when you're preparing input for text generation tasks, as it provides a clear starting point for the model's output.\n        - If set to \"False\", the function would format the conversation without adding this final prompt for generation.<br><br>\n        \n      This parameter helps in structuring the input in a way that's optimal for the model to understand where it should begin its response in the conversation flow.\n        \n    - \"add_special_tokens (bool, optional, defaults to True)\" â€” Whether or not to add special tokens when encoding the sequences. This will use the underlying \"PretrainedTokenizerBase.build_inputs_with_special_tokens\" function, which defines which tokens are automatically added to the input ids. This is usefull if you want to add bos or eos tokens automatically.\n    \n    - In some examples \"return_tensors='pt'\" is also in \"apply_chat_template()\". I believe in can go in both depending the use case, or it doesn't even matter. Sources:\n    \n        - https://huggingface.co/google/gemma-2-9b-it\n        - https://huggingface.co/docs/transformers/main/en/chat_templating","metadata":{"execution":{"iopub.status.busy":"2024-07-07T02:24:46.634122Z","iopub.execute_input":"2024-07-07T02:24:46.634488Z","iopub.status.idle":"2024-07-07T02:24:46.640817Z","shell.execute_reply.started":"2024-07-07T02:24:46.634459Z","shell.execute_reply":"2024-07-07T02:24:46.639498Z"}}},{"cell_type":"code","source":"max_new_tokens: int = 1024\ntop_p: float = 0.9\ntop_k: int = 50\ntemperature: float = 0.6\nrepetition_penalty: float = 1.2\n        \noutputs = model.generate(\n    input_ids=inputs.to(model.device), \n    do_sample=True,\n    max_new_tokens=max_new_tokens,\n    top_p=top_p,\n    top_k=top_k,\n    temperature=temperature,\n    repetition_penalty=repetition_penalty\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:59:07.146240Z","iopub.execute_input":"2024-07-07T23:59:07.147103Z","iopub.status.idle":"2024-07-07T23:59:31.905197Z","shell.execute_reply.started":"2024-07-07T23:59:07.147074Z","shell.execute_reply":"2024-07-07T23:59:31.904387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Notes**\n\n    \"generate()\" parameters: https://huggingface.co/docs/transformers/en/main_classes/text_generation","metadata":{}},{"cell_type":"code","source":"print(tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0])\n#print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:59:31.906361Z","iopub.execute_input":"2024-07-07T23:59:31.906931Z","iopub.status.idle":"2024-07-07T23:59:31.912479Z","shell.execute_reply.started":"2024-07-07T23:59:31.906903Z","shell.execute_reply":"2024-07-07T23:59:31.911633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## **Streamer Method**","metadata":{}},{"cell_type":"code","source":"from transformers import TextStreamer\n\nchat = [\"Who is better u or Qwen?\"]\ninputs = tokenizer(chat, return_tensors=\"pt\")\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n    \n_ = model.generate(\n    **inputs,\n    streamer=streamer,\n    do_sample=True,\n    max_new_tokens=max_new_tokens,\n    top_p=top_p,\n    top_k=top_k,\n    temperature=temperature,\n    num_beams=1,\n    repetition_penalty=repetition_penalty\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:59:31.913562Z","iopub.execute_input":"2024-07-07T23:59:31.913866Z","iopub.status.idle":"2024-07-07T23:59:45.584608Z","shell.execute_reply.started":"2024-07-07T23:59:31.913842Z","shell.execute_reply":"2024-07-07T23:59:45.583789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Source**\n\n    https://huggingface.co/docs/transformers/en/internal/generation_utils","metadata":{}},{"cell_type":"markdown","source":"> ## **Chat History**","metadata":{}},{"cell_type":"code","source":"from transformers import TextIteratorStreamer\nfrom threading import Thread\nimport os\n\nMAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\nchat_history: list[tuple[str, str]] = []\nconversation = []\nmessage = 'Okey, tell me a joke about Elon Musk'\n\nfor user, assistant in chat_history:\n    conversation.extend([\n        {\"role\": \"user\", \"content\": user},\n        {\"role\": \"assistant\", \"content\": assistant}\n    ])\n    \nconversation.append({\"role\": \"user\", \"content\": message})\ninput_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\")\n\nif input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n    input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n    print(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n\ninput_ids = input_ids.to(model.device)\nstreamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n\ngenerate_kwargs = dict(\n    {\"input_ids\": input_ids},\n    streamer=streamer,\n    max_new_tokens=max_new_tokens,\n    do_sample=True,\n    top_p=top_p,\n    top_k=top_k,\n    temperature=temperature,\n    num_beams=1,\n    repetition_penalty=repetition_penalty\n)\n\nt = Thread(target=model.generate, kwargs=generate_kwargs)\nt.start()\n\noutputs = []\n\nfor text in streamer:\n    outputs.append(text)\n    \nfinal_output = \"\".join(outputs)\nchat_history.append((message, final_output))\nprint(final_output)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:59:45.585836Z","iopub.execute_input":"2024-07-07T23:59:45.586201Z","iopub.status.idle":"2024-07-07T23:59:49.391348Z","shell.execute_reply.started":"2024-07-07T23:59:45.586158Z","shell.execute_reply":"2024-07-07T23:59:49.390419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:59:49.392346Z","iopub.execute_input":"2024-07-07T23:59:49.392610Z","iopub.status.idle":"2024-07-07T23:59:49.397336Z","shell.execute_reply.started":"2024-07-07T23:59:49.392587Z","shell.execute_reply":"2024-07-07T23:59:49.396497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(conversation)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:59:49.398342Z","iopub.execute_input":"2024-07-07T23:59:49.398612Z","iopub.status.idle":"2024-07-07T23:59:49.437274Z","shell.execute_reply.started":"2024-07-07T23:59:49.398588Z","shell.execute_reply":"2024-07-07T23:59:49.436385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conversation = []\nmessage = 'Now, tell me a story about Yann LeCun'    \n\nfor user, assistant in chat_history:\n    conversation.extend([\n        {\"role\": \"user\", \"content\": user},\n        {\"role\": \"assistant\", \"content\": assistant}\n    ])\n\nconversation.append({\"role\": \"user\", \"content\": message})\ninput_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\")\n\nif input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n    input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n    print(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n\ninput_ids = input_ids.to(model.device)\nstreamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n\ngenerate_kwargs = dict(\n    {\"input_ids\": input_ids},\n    streamer=streamer,\n    max_new_tokens=max_new_tokens,\n    do_sample=True,\n    top_p=top_p,\n    top_k=top_k,\n    temperature=temperature,\n    num_beams=1,\n    repetition_penalty=repetition_penalty\n)\n\nt = Thread(target=model.generate, kwargs=generate_kwargs)\nt.start()\n\noutputs = []\n\nfor text in streamer:\n    outputs.append(text)\n    \nfinal_output = \"\".join(outputs)\nchat_history.append((message, final_output))\nprint(final_output)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T23:59:49.438521Z","iopub.execute_input":"2024-07-07T23:59:49.438934Z","iopub.status.idle":"2024-07-08T00:00:32.736707Z","shell.execute_reply.started":"2024-07-07T23:59:49.438907Z","shell.execute_reply":"2024-07-08T00:00:32.735816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:00:32.737857Z","iopub.execute_input":"2024-07-08T00:00:32.738148Z","iopub.status.idle":"2024-07-08T00:00:32.743080Z","shell.execute_reply.started":"2024-07-08T00:00:32.738110Z","shell.execute_reply":"2024-07-08T00:00:32.742144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(conversation)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:00:32.746945Z","iopub.execute_input":"2024-07-08T00:00:32.747200Z","iopub.status.idle":"2024-07-08T00:00:32.756728Z","shell.execute_reply.started":"2024-07-08T00:00:32.747171Z","shell.execute_reply":"2024-07-08T00:00:32.755816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## **Function**","metadata":{}},{"cell_type":"code","source":"def generate_response(\n    message: str,\n    chat_history: list[tuple[str, str]],\n    max_new_tokens: int = 1024,\n    temperature: float = 0.6,\n    top_p: float = 0.9,\n    top_k: int = 50,\n    repetition_penalty: float = 1.2\n):\n    \n    conversation = []\n    \n    for user, assistant in chat_history:\n        conversation.extend([\n            {\"role\": \"user\", \"content\": user},\n            {\"role\": \"assistant\", \"content\": assistant}\n        ])\n    \n    conversation.append({\"role\": \"user\", \"content\": message})\n    input_ids = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, return_tensors=\"pt\")\n    \n    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n        print(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n    \n    input_ids = input_ids.to(model.device)\n    streamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n    \n    generate_kwargs = dict(\n        {\"input_ids\": input_ids},\n        streamer=streamer,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        top_p=top_p,\n        top_k=top_k,\n        temperature=temperature,\n        num_beams=1,\n        repetition_penalty=repetition_penalty\n    )\n    \n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n    \n    outputs = []\n    for text in streamer:\n        outputs.append(text)\n    \n    final_output = \"\".join(outputs)\n    chat_history.append((message, final_output))\n    \n    return final_output","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:00:32.757672Z","iopub.execute_input":"2024-07-08T00:00:32.757941Z","iopub.status.idle":"2024-07-08T00:00:32.768549Z","shell.execute_reply.started":"2024-07-08T00:00:32.757918Z","shell.execute_reply":"2024-07-08T00:00:32.767713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\nchat_history = []\n\nmessage = 'Tell me about the paper \"Attention Is All You Need\".'\nresponse = generate_response(message, chat_history)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:00:32.769578Z","iopub.execute_input":"2024-07-08T00:00:32.769868Z","iopub.status.idle":"2024-07-08T00:01:14.909758Z","shell.execute_reply.started":"2024-07-08T00:00:32.769846Z","shell.execute_reply":"2024-07-08T00:01:14.908756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"message = 'What paper did I ask you about?'\nresponse = generate_response(message, chat_history)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:01:14.910919Z","iopub.execute_input":"2024-07-08T00:01:14.911219Z","iopub.status.idle":"2024-07-08T00:01:23.216329Z","shell.execute_reply.started":"2024-07-08T00:01:14.911193Z","shell.execute_reply":"2024-07-08T00:01:23.215331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T00:01:23.217419Z","iopub.execute_input":"2024-07-08T00:01:23.217727Z","iopub.status.idle":"2024-07-08T00:01:23.222639Z","shell.execute_reply.started":"2024-07-08T00:01:23.217700Z","shell.execute_reply":"2024-07-08T00:01:23.221585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}